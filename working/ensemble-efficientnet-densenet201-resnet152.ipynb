{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "If you look at results of a big machine learning competition, you will most likely find that the top results are achieved by an ensemble of models rather than a single model. For instance, the top-scoring single model architecture at ILSVRC2015 is on place 13. Places 1–12 are taken by various ensembles.\n",
    "\n",
    "I haven’t seen a tutorial or documentation on how to use multiple neural networks in an ensemble, so I decided to make a practical guide on this topic.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*fy-6esoTWsTutld4fdSyCQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten,  Dropout, BatchNormalization, LeakyReLU,Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from kaggle_datasets import KaggleDatasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# TPU or GPU detection\n",
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    \n",
    "def seed_everything(seed=0):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "\n",
    "seed = 2020\n",
    "seed_everything(seed)\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n",
    "\n",
    "# Data access\n",
    "GCS_DS_PATH = KaggleDatasets().get_gcs_path()\n",
    "AUTO = tf.data.experimental.AUTOTUNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "EPOCHS = 35\n",
    "BATCH_SIZE = 6 * strategy.num_replicas_in_sync\n",
    "IMG_SIZE = 768\n",
    "print(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and perform test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_path(st):\n",
    "    return GCS_DS_PATH + '/images/' + st + '.jpg'\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/test.csv')\n",
    "sub = pd.read_csv('/kaggle/input/plant-pathology-2020-fgvc7/sample_submission.csv')\n",
    "\n",
    "train_paths = train.image_id.apply(format_path).values\n",
    "test_paths = test.image_id.apply(format_path).values\n",
    "train_labels = train.loc[:, 'healthy':].values\n",
    "SPLIT_VALIDATION =True\n",
    "if SPLIT_VALIDATION:\n",
    "    train_paths, valid_paths, train_labels, valid_labels =train_test_split(train_paths, train_labels, test_size=0.02, random_state=seed)\n",
    "\n",
    "def decode_image(filename, label=None, IMG_SIZE=(IMG_SIZE, IMG_SIZE)):\n",
    "    bits = tf.io.read_file(filename)\n",
    "    image = tf.image.decode_jpeg(bits, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    image = tf.image.central_crop(image, 0.9)\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n",
    "\n",
    "def data_augment(image, label=None):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_flip_up_down(image)\n",
    "    \n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "tf.data.Dataset\n",
    "    .from_tensor_slices((train_paths, train_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .cache()\n",
    "    .map(data_augment, num_parallel_calls=AUTO)\n",
    "    .repeat()\n",
    "    .shuffle(512)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "# train_dataset_1 = (\n",
    "# tf.data.Dataset\n",
    "#     .from_tensor_slices((train_paths, train_labels))\n",
    "#     .map(decode_image, num_parallel_calls=AUTO)\n",
    "#     .cache()\n",
    "#     .map(data_augment, num_parallel_calls=AUTO)\n",
    "#     .repeat()\n",
    "#     .shuffle(512)\n",
    "#     .batch(BATCH_SIZE)\n",
    "#     .prefetch(AUTO)\n",
    "# )\n",
    "valid_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices((valid_paths, valid_labels))\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .cache()\n",
    "    .prefetch(AUTO)\n",
    ")\n",
    "\n",
    "test_dataset = (\n",
    "    tf.data.Dataset\n",
    "    .from_tensor_slices(test_paths)\n",
    "    .map(decode_image, num_parallel_calls=AUTO)\n",
    "    .map(data_augment, num_parallel_calls=AUTO)\n",
    "    .batch(BATCH_SIZE)\n",
    ")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate schedule: 0.0001 to 0.0004 to 0.000104\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "LR_START = 0.0001\n",
    "LR_MAX = 0.00005 * strategy.num_replicas_in_sync\n",
    "LR_MIN = 0.0001\n",
    "LR_RAMPUP_EPOCHS = 12\n",
    "LR_SUSTAIN_EPOCHS = 3\n",
    "LR_EXP_DECAY = .8\n",
    "\n",
    "def lrfn(epoch):\n",
    "    if epoch < LR_RAMPUP_EPOCHS:\n",
    "        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n",
    "    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n",
    "        lr = LR_MAX\n",
    "    else:\n",
    "        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n",
    "    return lr\n",
    "    \n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n",
    "\n",
    "rng = [i for i in range(EPOCHS)]\n",
    "y = [lrfn(x) for x in rng]\n",
    "plt.plot(rng, y)\n",
    "print(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet\n",
    "\n",
    "EfficientNet, which not only focuses on improving the accuracy, but also the efficiency of models.\n",
    "\n",
    "### What does scaling mean in the context of CNNs?\n",
    "\n",
    "There are three scaling dimensions of a CNN: depth, width, and resolution. Depth simply means how deep the networks is which is equivalent to the number of layers in it. Width simply means how wide the network is. One measure of width, for example, is the number of channels in a Conv layer whereas Resolution is simply the image resolution that is being passed to a CNN. The figure below(from the paper itself) will give you a clear idea of what scaling means across different dimensions. We will discuss these in detail as well.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*xQCVt1tFWe7XNWVEmC6hGQ.png)\n",
    "\n",
    "EfficientNet Architecture\n",
    "\n",
    "Scaling doesn’t change the layer operations, hence it is better to first have a good baseline network and then scale it along different dimensions using the proposed compound scaling. The authors obtained their base network by doing a Neural Architecture Search (NAS) that optimizes for both accuracy and FLOPS. The architecture is similar to M-NASNet as it has been found using the similar search space. The network layers/blocks are as shown below:\n",
    "![](https://miro.medium.com/max/1400/1*OpvSpqMP61IO_9cp4mAXnA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b7_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "258441216/258434480 [==============================] - 3s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "!pip install -q efficientnet\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "\n",
    "\n",
    "with strategy.scope():    \n",
    "    efficient_net = efn.EfficientNetB7(\n",
    "                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                    weights='imagenet',\n",
    "                    include_top=False\n",
    "                    )\n",
    "    x = efficient_net.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "    model =  keras.Model(inputs = efficient_net.input,outputs=x)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 37 steps, validate for 1 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/35\n",
      "37/37 [==============================] - 422s 11s/step - loss: 0.8570 - accuracy: 0.7100 - val_loss: 0.7310 - val_accuracy: 0.7297\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.000125.\n",
      "Epoch 2/35\n",
      "37/37 [==============================] - 41s 1s/step - loss: 0.3811 - accuracy: 0.8806 - val_loss: 0.2566 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00015000000000000001.\n",
      "Epoch 3/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.3105 - accuracy: 0.9009 - val_loss: 0.1284 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00017500000000000003.\n",
      "Epoch 4/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.2432 - accuracy: 0.9251 - val_loss: 0.2641 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 5/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.2080 - accuracy: 0.9279 - val_loss: 0.0921 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.000225.\n",
      "Epoch 6/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1906 - accuracy: 0.9336 - val_loss: 0.1630 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 7/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1616 - accuracy: 0.9488 - val_loss: 0.0244 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000275.\n",
      "Epoch 8/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1401 - accuracy: 0.9578 - val_loss: 0.1354 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00030000000000000003.\n",
      "Epoch 9/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1358 - accuracy: 0.9617 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00032500000000000004.\n",
      "Epoch 10/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1088 - accuracy: 0.9696 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00035.\n",
      "Epoch 11/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1334 - accuracy: 0.9578 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000375.\n",
      "Epoch 12/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1031 - accuracy: 0.9628 - val_loss: 0.0117 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 13/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.1285 - accuracy: 0.9617 - val_loss: 0.0211 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 14/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0963 - accuracy: 0.9679 - val_loss: 0.0815 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 15/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0931 - accuracy: 0.9696 - val_loss: 0.0281 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 16/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0661 - accuracy: 0.9786 - val_loss: 0.0748 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00034.\n",
      "Epoch 17/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0669 - accuracy: 0.9775 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00029200000000000005.\n",
      "Epoch 18/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0715 - accuracy: 0.9780 - val_loss: 0.0486 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00025360000000000004.\n",
      "Epoch 19/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0370 - accuracy: 0.9876 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00022288000000000006.\n",
      "Epoch 20/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0363 - accuracy: 0.9899 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00019830400000000006.\n",
      "Epoch 21/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00017864320000000004.\n",
      "Epoch 22/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0286 - accuracy: 0.9910 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00016291456000000005.\n",
      "Epoch 23/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0120 - accuracy: 0.9972 - val_loss: 4.6118e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00015033164800000003.\n",
      "Epoch 24/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0132 - accuracy: 0.9949 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00014026531840000004.\n",
      "Epoch 25/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00013221225472000002.\n",
      "Epoch 26/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0174 - accuracy: 0.9955 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.00012576980377600002.\n",
      "Epoch 27/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0089 - accuracy: 0.9977 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00012061584302080001.\n",
      "Epoch 28/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 4.6618e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00011649267441664002.\n",
      "Epoch 29/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0061 - accuracy: 0.9983 - val_loss: 2.8832e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00011319413953331202.\n",
      "Epoch 30/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 2.1716e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00011055531162664962.\n",
      "Epoch 31/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0091 - accuracy: 0.9977 - val_loss: 2.8606e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0001084442493013197.\n",
      "Epoch 32/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 3.3054e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010675539944105576.\n",
      "Epoch 33/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0079 - accuracy: 0.9989 - val_loss: 0.0014 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0001054043195528446.\n",
      "Epoch 34/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 5.9335e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00010432345564227568.\n",
      "Epoch 35/35\n",
      "37/37 [==============================] - 40s 1s/step - loss: 0.0087 - accuracy: 0.9983 - val_loss: 5.4595e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>1.383620e-04</td>\n",
       "      <td>7.273175e-05</td>\n",
       "      <td>9.997773e-01</td>\n",
       "      <td>1.165818e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>8.223471e-08</td>\n",
       "      <td>4.283033e-07</td>\n",
       "      <td>9.999995e-01</td>\n",
       "      <td>3.074289e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>2.720899e-06</td>\n",
       "      <td>4.448625e-06</td>\n",
       "      <td>4.184871e-07</td>\n",
       "      <td>9.999924e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>9.999863e-01</td>\n",
       "      <td>1.683302e-07</td>\n",
       "      <td>1.096132e-05</td>\n",
       "      <td>2.593581e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>1.275523e-05</td>\n",
       "      <td>6.466059e-05</td>\n",
       "      <td>9.999192e-01</td>\n",
       "      <td>3.484726e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>Test_1816</td>\n",
       "      <td>1.221290e-06</td>\n",
       "      <td>3.023820e-06</td>\n",
       "      <td>9.999933e-01</td>\n",
       "      <td>2.416008e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>Test_1817</td>\n",
       "      <td>3.834428e-08</td>\n",
       "      <td>1.748531e-04</td>\n",
       "      <td>5.915035e-11</td>\n",
       "      <td>9.998252e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>Test_1818</td>\n",
       "      <td>6.883852e-07</td>\n",
       "      <td>1.398088e-06</td>\n",
       "      <td>9.999977e-01</td>\n",
       "      <td>7.714485e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>Test_1819</td>\n",
       "      <td>9.999968e-01</td>\n",
       "      <td>2.595357e-06</td>\n",
       "      <td>4.295934e-07</td>\n",
       "      <td>1.427688e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>Test_1820</td>\n",
       "      <td>2.752825e-07</td>\n",
       "      <td>3.673401e-04</td>\n",
       "      <td>9.876029e-10</td>\n",
       "      <td>9.996324e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id       healthy  multiple_diseases          rust          scab\n",
       "0        Test_0  1.383620e-04       7.273175e-05  9.997773e-01  1.165818e-05\n",
       "1        Test_1  8.223471e-08       4.283033e-07  9.999995e-01  3.074289e-08\n",
       "2        Test_2  2.720899e-06       4.448625e-06  4.184871e-07  9.999924e-01\n",
       "3        Test_3  9.999863e-01       1.683302e-07  1.096132e-05  2.593581e-06\n",
       "4        Test_4  1.275523e-05       6.466059e-05  9.999192e-01  3.484726e-06\n",
       "...         ...           ...                ...           ...           ...\n",
       "1816  Test_1816  1.221290e-06       3.023820e-06  9.999933e-01  2.416008e-06\n",
       "1817  Test_1817  3.834428e-08       1.748531e-04  5.915035e-11  9.998252e-01\n",
       "1818  Test_1818  6.883852e-07       1.398088e-06  9.999977e-01  7.714485e-08\n",
       "1819  Test_1819  9.999968e-01       2.595357e-06  4.295934e-07  1.427688e-07\n",
       "1820  Test_1820  2.752825e-07       3.673401e-04  9.876029e-10  9.996324e-01\n",
       "\n",
       "[1821 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict= model.predict(test_dataset)\n",
    "\n",
    "# prediction = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\n",
    "# for row in range(test.shape[0]):\n",
    "#     for col in range(4):\n",
    "#         if predict[row][col] == max(predict[row]):\n",
    "#             prediction[row][col] = 1\n",
    "#         else:\n",
    "#             prediction[row][col] = 0\n",
    "            \n",
    "prediction = pd.DataFrame(predict)\n",
    "prediction.columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "df = pd.concat([test.image_id, prediction], axis = 1)\n",
    "\n",
    "df.to_csv('effi_submission.csv', index = False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DenseNet 201\n",
    "\n",
    "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections — one between each layer and its subsequent layer — our network has L(L+1)/ 2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters.\n",
    "DenseNet architecture is given below\n",
    "![](https://miro.medium.com/max/1400/1*gAGIm5uaaUukJOSzMW6saQ.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "74842112/74836368 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2,ResNet101V2,ResNet152V2,DenseNet201\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "with strategy.scope():    \n",
    "    Dense_net = DenseNet201(\n",
    "                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                    weights='imagenet',\n",
    "                    include_top=False\n",
    "                    )\n",
    "    x = Dense_net.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "    model =  keras.Model(inputs = Dense_net.input,outputs=x)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 37 steps, validate for 1 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/35\n",
      "37/37 [==============================] - 419s 11s/step - loss: 0.6265 - accuracy: 0.7635 - val_loss: 1.5909 - val_accuracy: 0.3784\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.000125.\n",
      "Epoch 2/35\n",
      "37/37 [==============================] - 25s 679ms/step - loss: 0.3194 - accuracy: 0.8885 - val_loss: 1.0450 - val_accuracy: 0.5676\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00015000000000000001.\n",
      "Epoch 3/35\n",
      "37/37 [==============================] - 25s 681ms/step - loss: 0.2288 - accuracy: 0.9217 - val_loss: 0.3499 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00017500000000000003.\n",
      "Epoch 4/35\n",
      "37/37 [==============================] - 25s 678ms/step - loss: 0.1771 - accuracy: 0.9437 - val_loss: 0.1489 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 5/35\n",
      "37/37 [==============================] - 25s 680ms/step - loss: 0.1669 - accuracy: 0.9516 - val_loss: 0.1217 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.000225.\n",
      "Epoch 6/35\n",
      "37/37 [==============================] - 25s 680ms/step - loss: 0.1573 - accuracy: 0.9505 - val_loss: 0.0380 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 7/35\n",
      "37/37 [==============================] - 25s 677ms/step - loss: 0.1397 - accuracy: 0.9578 - val_loss: 0.5496 - val_accuracy: 0.7568\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000275.\n",
      "Epoch 8/35\n",
      "37/37 [==============================] - 25s 683ms/step - loss: 0.1552 - accuracy: 0.9566 - val_loss: 0.3454 - val_accuracy: 0.8378\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00030000000000000003.\n",
      "Epoch 9/35\n",
      "37/37 [==============================] - 25s 681ms/step - loss: 0.1052 - accuracy: 0.9673 - val_loss: 0.1718 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00032500000000000004.\n",
      "Epoch 10/35\n",
      "37/37 [==============================] - 25s 678ms/step - loss: 0.1062 - accuracy: 0.9685 - val_loss: 0.3824 - val_accuracy: 0.8378\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00035.\n",
      "Epoch 11/35\n",
      "37/37 [==============================] - 25s 677ms/step - loss: 0.1169 - accuracy: 0.9611 - val_loss: 0.3066 - val_accuracy: 0.8919\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000375.\n",
      "Epoch 12/35\n",
      "37/37 [==============================] - 25s 681ms/step - loss: 0.1035 - accuracy: 0.9730 - val_loss: 0.5022 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 13/35\n",
      "37/37 [==============================] - 25s 678ms/step - loss: 0.1529 - accuracy: 0.9493 - val_loss: 0.0879 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 14/35\n",
      "37/37 [==============================] - 25s 679ms/step - loss: 0.1297 - accuracy: 0.9651 - val_loss: 0.0808 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 15/35\n",
      "37/37 [==============================] - 25s 677ms/step - loss: 0.1083 - accuracy: 0.9673 - val_loss: 0.1618 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 16/35\n",
      "37/37 [==============================] - 25s 679ms/step - loss: 0.1402 - accuracy: 0.9572 - val_loss: 0.1105 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00034.\n",
      "Epoch 17/35\n",
      "37/37 [==============================] - 25s 682ms/step - loss: 0.0776 - accuracy: 0.9792 - val_loss: 0.0378 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00029200000000000005.\n",
      "Epoch 18/35\n",
      "37/37 [==============================] - 25s 682ms/step - loss: 0.0441 - accuracy: 0.9876 - val_loss: 0.0838 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00025360000000000004.\n",
      "Epoch 19/35\n",
      "37/37 [==============================] - 25s 680ms/step - loss: 0.0472 - accuracy: 0.9870 - val_loss: 0.0282 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00022288000000000006.\n",
      "Epoch 20/35\n",
      "37/37 [==============================] - 25s 674ms/step - loss: 0.0416 - accuracy: 0.9899 - val_loss: 0.0234 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00019830400000000006.\n",
      "Epoch 21/35\n",
      "37/37 [==============================] - 25s 677ms/step - loss: 0.0212 - accuracy: 0.9961 - val_loss: 0.0520 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00017864320000000004.\n",
      "Epoch 22/35\n",
      "37/37 [==============================] - 25s 677ms/step - loss: 0.0192 - accuracy: 0.9944 - val_loss: 0.0404 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00016291456000000005.\n",
      "Epoch 23/35\n",
      "37/37 [==============================] - 25s 680ms/step - loss: 0.0125 - accuracy: 0.9983 - val_loss: 0.0065 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00015033164800000003.\n",
      "Epoch 24/35\n",
      "37/37 [==============================] - 25s 677ms/step - loss: 0.0280 - accuracy: 0.9944 - val_loss: 0.0149 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00014026531840000004.\n",
      "Epoch 25/35\n",
      "37/37 [==============================] - 25s 679ms/step - loss: 0.0165 - accuracy: 0.9955 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00013221225472000002.\n",
      "Epoch 26/35\n",
      "37/37 [==============================] - 25s 682ms/step - loss: 0.0220 - accuracy: 0.9944 - val_loss: 0.0436 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.00012576980377600002.\n",
      "Epoch 27/35\n",
      "37/37 [==============================] - 25s 680ms/step - loss: 0.0081 - accuracy: 0.9989 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00012061584302080001.\n",
      "Epoch 28/35\n",
      "37/37 [==============================] - 25s 679ms/step - loss: 0.0150 - accuracy: 0.9977 - val_loss: 0.0376 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00011649267441664002.\n",
      "Epoch 29/35\n",
      "37/37 [==============================] - 27s 724ms/step - loss: 0.0151 - accuracy: 0.9977 - val_loss: 0.0198 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00011319413953331202.\n",
      "Epoch 30/35\n",
      "37/37 [==============================] - 25s 679ms/step - loss: 0.0113 - accuracy: 0.9983 - val_loss: 0.0219 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00011055531162664962.\n",
      "Epoch 31/35\n",
      "37/37 [==============================] - 25s 678ms/step - loss: 0.0078 - accuracy: 0.9977 - val_loss: 0.0130 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0001084442493013197.\n",
      "Epoch 32/35\n",
      "37/37 [==============================] - 25s 678ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 0.0135 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010675539944105576.\n",
      "Epoch 33/35\n",
      "37/37 [==============================] - 25s 681ms/step - loss: 0.0109 - accuracy: 0.9977 - val_loss: 0.0428 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0001054043195528446.\n",
      "Epoch 34/35\n",
      "37/37 [==============================] - 25s 678ms/step - loss: 0.0072 - accuracy: 0.9989 - val_loss: 0.0160 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00010432345564227568.\n",
      "Epoch 35/35\n",
      "37/37 [==============================] - 25s 679ms/step - loss: 0.0132 - accuracy: 0.9966 - val_loss: 0.0232 - val_accuracy: 0.9730\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>6.548931e-06</td>\n",
       "      <td>0.015685</td>\n",
       "      <td>0.984202</td>\n",
       "      <td>1.059167e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>3.269162e-06</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.999026</td>\n",
       "      <td>2.555017e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>5.171888e-06</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>9.999614e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>9.930336e-01</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>2.304812e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>3.594242e-07</td>\n",
       "      <td>0.043991</td>\n",
       "      <td>0.956007</td>\n",
       "      <td>9.214993e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>Test_1816</td>\n",
       "      <td>1.185579e-07</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.998431</td>\n",
       "      <td>8.133044e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>Test_1817</td>\n",
       "      <td>4.044929e-03</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>9.836838e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>Test_1818</td>\n",
       "      <td>1.872135e-04</td>\n",
       "      <td>0.119130</td>\n",
       "      <td>0.880608</td>\n",
       "      <td>7.460907e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>Test_1819</td>\n",
       "      <td>9.994660e-01</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>4.935252e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>Test_1820</td>\n",
       "      <td>5.218833e-05</td>\n",
       "      <td>0.008666</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>9.912537e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id       healthy  multiple_diseases      rust          scab\n",
       "0        Test_0  6.548931e-06           0.015685  0.984202  1.059167e-04\n",
       "1        Test_1  3.269162e-06           0.000969  0.999026  2.555017e-06\n",
       "2        Test_2  5.171888e-06           0.000027  0.000006  9.999614e-01\n",
       "3        Test_3  9.930336e-01           0.000018  0.004644  2.304812e-03\n",
       "4        Test_4  3.594242e-07           0.043991  0.956007  9.214993e-07\n",
       "...         ...           ...                ...       ...           ...\n",
       "1816  Test_1816  1.185579e-07           0.001561  0.998431  8.133044e-06\n",
       "1817  Test_1817  4.044929e-03           0.012104  0.000167  9.836838e-01\n",
       "1818  Test_1818  1.872135e-04           0.119130  0.880608  7.460907e-05\n",
       "1819  Test_1819  9.994660e-01           0.000037  0.000004  4.935252e-04\n",
       "1820  Test_1820  5.218833e-05           0.008666  0.000029  9.912537e-01\n",
       "\n",
       "[1821 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict= model.predict(test_dataset)\n",
    "\n",
    "# prediction = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\n",
    "# for row in range(test.shape[0]):\n",
    "#     for col in range(4):\n",
    "#         if predict[row][col] == max(predict[row]):\n",
    "#             prediction[row][col] = 1\n",
    "#         else:\n",
    "#             prediction[row][col] = 0\n",
    "            \n",
    "prediction = pd.DataFrame(predict)\n",
    "prediction.columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "df_dense = pd.concat([test.image_id, prediction], axis = 1)\n",
    "\n",
    "df_dense.to_csv('dense_submission.csv', index = False)\n",
    "df_dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet152\n",
    "ResNet introduces skip connection (or shortcut connection) to fit the input from the previous layer to the next layer without any modification of the input. Skip connection enables to have deeper network and finally ResNet becomes the Winner of ILSVRC 2015 in image classification, detection, and localization, as well as Winner of MS COCO 2015 detection, and segmentation. This is a 2016 CVPR paper with more than 19000 citations.\n",
    "\n",
    "Skip / Shortcut Connection in Residual Network (ResNet)\n",
    "\n",
    "To solve the problem of vanishing/exploding gradients, a skip / shortcut connection is added to add the input x to the output after few weight layers as below:\n",
    "![](https://miro.medium.com/max/894/1*rbhjv7ZdAgXM2MlBUL5Mmw.png)\n",
    "\n",
    "Hence, the output H(x)= F(x) + x. The weight layers actually is to learn a kind of residual mapping: F(x)=H(x)-x.\n",
    "\n",
    "Even if there is vanishing gradient for the weight layers, we always still have the identity x to transfer back to earlier layers.\n",
    "Resnet Architecture:\n",
    "![](https://miro.medium.com/max/2000/1*6hF97Upuqg_LdsqWY6n_wg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b5_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "115515392/115515256 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50V2,ResNet101V2,ResNet152V2,DenseNet201\n",
    "from keras.models import Model\n",
    "from tensorflow import keras\n",
    "with strategy.scope():\n",
    "    efficient_net = efn.EfficientNetB5(\n",
    "                    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "                    weights='imagenet',\n",
    "                    include_top=False\n",
    "                    )\n",
    "    x = efficient_net.output\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "    model =  keras.Model(inputs = efficient_net.input,outputs=x)\n",
    "    \n",
    "#     Res_net = ResNet152V2(\n",
    "#                     input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "#                     weights='imagenet',\n",
    "#                     include_top=False\n",
    "#                     )\n",
    "#     x = Res_net.output\n",
    "#     x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "#     x = tf.keras.layers.Dense(4, activation='softmax')(x)\n",
    "#     model =  keras.Model(inputs = Res_net.input,outputs=x)\n",
    "\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer= 'adam', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 37 steps, validate for 1 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.0001.\n",
      "Epoch 1/35\n",
      "37/37 [==============================] - 272s 7s/step - loss: 0.7801 - accuracy: 0.7494 - val_loss: 0.6714 - val_accuracy: 0.8108\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.000125.\n",
      "Epoch 2/35\n",
      "37/37 [==============================] - 24s 655ms/step - loss: 0.3669 - accuracy: 0.8733 - val_loss: 0.2500 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.00015000000000000001.\n",
      "Epoch 3/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.2670 - accuracy: 0.9122 - val_loss: 0.2010 - val_accuracy: 0.9459\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.00017500000000000003.\n",
      "Epoch 4/35\n",
      "37/37 [==============================] - 24s 657ms/step - loss: 0.2276 - accuracy: 0.9307 - val_loss: 0.1630 - val_accuracy: 0.9189\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.0002.\n",
      "Epoch 5/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.1998 - accuracy: 0.9409 - val_loss: 0.0631 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00006: LearningRateScheduler reducing learning rate to 0.000225.\n",
      "Epoch 6/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.1369 - accuracy: 0.9578 - val_loss: 0.0227 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00007: LearningRateScheduler reducing learning rate to 0.00025.\n",
      "Epoch 7/35\n",
      "37/37 [==============================] - 24s 655ms/step - loss: 0.1005 - accuracy: 0.9679 - val_loss: 0.0032 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00008: LearningRateScheduler reducing learning rate to 0.000275.\n",
      "Epoch 8/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.1129 - accuracy: 0.9651 - val_loss: 0.0488 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00009: LearningRateScheduler reducing learning rate to 0.00030000000000000003.\n",
      "Epoch 9/35\n",
      "37/37 [==============================] - 24s 656ms/step - loss: 0.0985 - accuracy: 0.9702 - val_loss: 0.0094 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00010: LearningRateScheduler reducing learning rate to 0.00032500000000000004.\n",
      "Epoch 10/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.0870 - accuracy: 0.9741 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00011: LearningRateScheduler reducing learning rate to 0.00035.\n",
      "Epoch 11/35\n",
      "37/37 [==============================] - 24s 651ms/step - loss: 0.0874 - accuracy: 0.9730 - val_loss: 0.0287 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00012: LearningRateScheduler reducing learning rate to 0.000375.\n",
      "Epoch 12/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.0825 - accuracy: 0.9730 - val_loss: 0.0539 - val_accuracy: 0.9730\n",
      "\n",
      "Epoch 00013: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 13/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.0637 - accuracy: 0.9814 - val_loss: 4.1667e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00014: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 14/35\n",
      "37/37 [==============================] - 24s 651ms/step - loss: 0.0402 - accuracy: 0.9887 - val_loss: 1.9748e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00015: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 15/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.0477 - accuracy: 0.9882 - val_loss: 0.0011 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00016: LearningRateScheduler reducing learning rate to 0.0004.\n",
      "Epoch 16/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.0570 - accuracy: 0.9865 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00017: LearningRateScheduler reducing learning rate to 0.00034.\n",
      "Epoch 17/35\n",
      "37/37 [==============================] - 24s 655ms/step - loss: 0.0533 - accuracy: 0.9797 - val_loss: 4.9042e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00018: LearningRateScheduler reducing learning rate to 0.00029200000000000005.\n",
      "Epoch 18/35\n",
      "37/37 [==============================] - 24s 651ms/step - loss: 0.0280 - accuracy: 0.9927 - val_loss: 1.8541e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00019: LearningRateScheduler reducing learning rate to 0.00025360000000000004.\n",
      "Epoch 19/35\n",
      "37/37 [==============================] - 24s 651ms/step - loss: 0.0258 - accuracy: 0.9932 - val_loss: 8.8126e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00020: LearningRateScheduler reducing learning rate to 0.00022288000000000006.\n",
      "Epoch 20/35\n",
      "37/37 [==============================] - 24s 659ms/step - loss: 0.0204 - accuracy: 0.9955 - val_loss: 1.5520e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00021: LearningRateScheduler reducing learning rate to 0.00019830400000000006.\n",
      "Epoch 21/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.0109 - accuracy: 0.9972 - val_loss: 2.3575e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00022: LearningRateScheduler reducing learning rate to 0.00017864320000000004.\n",
      "Epoch 22/35\n",
      "37/37 [==============================] - 24s 656ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 3.1460e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00023: LearningRateScheduler reducing learning rate to 0.00016291456000000005.\n",
      "Epoch 23/35\n",
      "37/37 [==============================] - 24s 655ms/step - loss: 0.0053 - accuracy: 0.9994 - val_loss: 2.1644e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00024: LearningRateScheduler reducing learning rate to 0.00015033164800000003.\n",
      "Epoch 24/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.0136 - accuracy: 0.9972 - val_loss: 2.7753e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00025: LearningRateScheduler reducing learning rate to 0.00014026531840000004.\n",
      "Epoch 25/35\n",
      "37/37 [==============================] - 24s 655ms/step - loss: 0.0115 - accuracy: 0.9972 - val_loss: 3.0890e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00026: LearningRateScheduler reducing learning rate to 0.00013221225472000002.\n",
      "Epoch 26/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.0112 - accuracy: 0.9972 - val_loss: 3.4219e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00027: LearningRateScheduler reducing learning rate to 0.00012576980377600002.\n",
      "Epoch 27/35\n",
      "37/37 [==============================] - 24s 653ms/step - loss: 0.0060 - accuracy: 0.9983 - val_loss: 1.5545e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00028: LearningRateScheduler reducing learning rate to 0.00012061584302080001.\n",
      "Epoch 28/35\n",
      "37/37 [==============================] - 24s 656ms/step - loss: 0.0072 - accuracy: 0.9983 - val_loss: 1.5614e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00029: LearningRateScheduler reducing learning rate to 0.00011649267441664002.\n",
      "Epoch 29/35\n",
      "37/37 [==============================] - 24s 650ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 2.0404e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00030: LearningRateScheduler reducing learning rate to 0.00011319413953331202.\n",
      "Epoch 30/35\n",
      "37/37 [==============================] - 24s 651ms/step - loss: 0.0081 - accuracy: 0.9977 - val_loss: 0.0106 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00031: LearningRateScheduler reducing learning rate to 0.00011055531162664962.\n",
      "Epoch 31/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 6.3753e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00032: LearningRateScheduler reducing learning rate to 0.0001084442493013197.\n",
      "Epoch 32/35\n",
      "37/37 [==============================] - 24s 654ms/step - loss: 0.0041 - accuracy: 0.9983 - val_loss: 7.9055e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00033: LearningRateScheduler reducing learning rate to 0.00010675539944105576.\n",
      "Epoch 33/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.0044 - accuracy: 0.9983 - val_loss: 4.3555e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00034: LearningRateScheduler reducing learning rate to 0.0001054043195528446.\n",
      "Epoch 34/35\n",
      "37/37 [==============================] - 24s 652ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 6.0568e-04 - val_accuracy: 1.0000\n",
      "\n",
      "Epoch 00035: LearningRateScheduler reducing learning rate to 0.00010432345564227568.\n",
      "Epoch 35/35\n",
      "37/37 [==============================] - 24s 656ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 3.7414e-04 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "STEPS_PER_EPOCH = train_labels.shape[0] // BATCH_SIZE\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset, \n",
    "    epochs=EPOCHS, \n",
    "    callbacks=[lr_callback],\n",
    "    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "    validation_data=valid_dataset if SPLIT_VALIDATION else None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Test_0</td>\n",
       "      <td>7.179494e-07</td>\n",
       "      <td>1.505928e-06</td>\n",
       "      <td>9.999976e-01</td>\n",
       "      <td>1.670584e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test_1</td>\n",
       "      <td>4.036785e-10</td>\n",
       "      <td>2.573168e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.165098e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Test_2</td>\n",
       "      <td>2.097709e-08</td>\n",
       "      <td>3.112994e-07</td>\n",
       "      <td>8.399684e-08</td>\n",
       "      <td>9.999996e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Test_3</td>\n",
       "      <td>9.999844e-01</td>\n",
       "      <td>9.460580e-09</td>\n",
       "      <td>1.535165e-05</td>\n",
       "      <td>2.461229e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Test_4</td>\n",
       "      <td>3.959292e-07</td>\n",
       "      <td>3.825877e-06</td>\n",
       "      <td>9.999958e-01</td>\n",
       "      <td>2.779181e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1816</th>\n",
       "      <td>Test_1816</td>\n",
       "      <td>2.797445e-08</td>\n",
       "      <td>3.247006e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.004308e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1817</th>\n",
       "      <td>Test_1817</td>\n",
       "      <td>7.564233e-06</td>\n",
       "      <td>4.995646e-05</td>\n",
       "      <td>1.694421e-05</td>\n",
       "      <td>9.999255e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1818</th>\n",
       "      <td>Test_1818</td>\n",
       "      <td>1.299998e-09</td>\n",
       "      <td>1.711463e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.511707e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1819</th>\n",
       "      <td>Test_1819</td>\n",
       "      <td>9.999993e-01</td>\n",
       "      <td>5.297334e-09</td>\n",
       "      <td>5.845930e-07</td>\n",
       "      <td>1.391459e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>Test_1820</td>\n",
       "      <td>1.269486e-05</td>\n",
       "      <td>2.397020e-04</td>\n",
       "      <td>6.627341e-07</td>\n",
       "      <td>9.997470e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id       healthy  multiple_diseases          rust          scab\n",
       "0        Test_0  7.179494e-07       1.505928e-06  9.999976e-01  1.670584e-07\n",
       "1        Test_1  4.036785e-10       2.573168e-09  1.000000e+00  3.165098e-10\n",
       "2        Test_2  2.097709e-08       3.112994e-07  8.399684e-08  9.999996e-01\n",
       "3        Test_3  9.999844e-01       9.460580e-09  1.535165e-05  2.461229e-07\n",
       "4        Test_4  3.959292e-07       3.825877e-06  9.999958e-01  2.779181e-08\n",
       "...         ...           ...                ...           ...           ...\n",
       "1816  Test_1816  2.797445e-08       3.247006e-09  1.000000e+00  3.004308e-08\n",
       "1817  Test_1817  7.564233e-06       4.995646e-05  1.694421e-05  9.999255e-01\n",
       "1818  Test_1818  1.299998e-09       1.711463e-09  1.000000e+00  5.511707e-13\n",
       "1819  Test_1819  9.999993e-01       5.297334e-09  5.845930e-07  1.391459e-07\n",
       "1820  Test_1820  1.269486e-05       2.397020e-04  6.627341e-07  9.997470e-01\n",
       "\n",
       "[1821 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict= model.predict(test_dataset)\n",
    "\n",
    "# prediction = np.ndarray(shape = (test.shape[0],4), dtype = np.float32)\n",
    "# for row in range(test.shape[0]):\n",
    "#     for col in range(4):\n",
    "#         if predict[row][col] == max(predict[row]):\n",
    "#             prediction[row][col] = 1\n",
    "#         else:\n",
    "#             prediction[row][col] = 0\n",
    "            \n",
    "prediction = pd.DataFrame(predict)\n",
    "prediction.columns = ['healthy', 'multiple_diseases', 'rust', 'scab']\n",
    "df_res = pd.concat([test.image_id, prediction], axis = 1)\n",
    "\n",
    "df_res.to_csv('res_submission.csv', index = False)\n",
    "df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voting(a,b,c):\n",
    "    return (a+0.8*b+0.9*c)/(1.0+0.8+0.9)\n",
    "    if a==b:\n",
    "        return a\n",
    "    if b==c:\n",
    "        return c\n",
    "    if a==c:\n",
    "        return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = df['image_id']\n",
    "healthy = []\n",
    "multiple_diseases = []\n",
    "rust = []\n",
    "scab = []\n",
    "for i in range(len(df['healthy'])):\n",
    "    healthy.append(voting(df['healthy'][i],df_dense['healthy'][i],df_res['healthy'][i]))\n",
    "    multiple_diseases.append(voting(df['multiple_diseases'][i],df_dense['multiple_diseases'][i],df_res['multiple_diseases'][i]))\n",
    "    rust.append(voting(df['rust'][i],df_dense['rust'][i],df_res['rust'][i]))\n",
    "    scab.append(voting(df['scab'][i],df_dense['scab'][i],df_res['scab'][i]))\n",
    "    \n",
    "finalsubmission = pd.DataFrame(columns = ['image_id','healthy', 'multiple_diseases', 'rust', 'scab'])\n",
    "\n",
    "finalsubmission['image_id'] = image_id\n",
    "finalsubmission['healthy'] = healthy\n",
    "finalsubmission['multiple_diseases'] = multiple_diseases\n",
    "finalsubmission['rust'] = rust\n",
    "finalsubmission['scab'] = scab\n",
    "finalsubmission.to_csv('submission.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
